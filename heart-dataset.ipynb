{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries. Libraries needed will be :\n","* numpy\n","* pandas\n","* seaborn\n","* plotly\n","* sklearn and its classifiers"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-13T06:24:51.202723Z","iopub.status.busy":"2021-12-13T06:24:51.202064Z","iopub.status.idle":"2021-12-13T06:24:53.617972Z","shell.execute_reply":"2021-12-13T06:24:53.617111Z","shell.execute_reply.started":"2021-12-13T06:24:51.202619Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np \n","import pandas as pd \n","import warnings\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","warnings.filterwarnings(\"ignore\")\n","pd.set_option(\"display.max_rows\",None)\n","from sklearn import preprocessing\n","import matplotlib \n","matplotlib.style.use('ggplot')\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.620345Z","iopub.status.busy":"2021-12-13T06:24:53.620027Z","iopub.status.idle":"2021-12-13T06:24:53.663223Z","shell.execute_reply":"2021-12-13T06:24:53.662567Z","shell.execute_reply.started":"2021-12-13T06:24:53.620313Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>ChestPainType</th>\n","      <th>RestingBP</th>\n","      <th>Cholesterol</th>\n","      <th>FastingBS</th>\n","      <th>RestingECG</th>\n","      <th>MaxHR</th>\n","      <th>ExerciseAngina</th>\n","      <th>Oldpeak</th>\n","      <th>ST_Slope</th>\n","      <th>HeartDisease</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>40</td>\n","      <td>M</td>\n","      <td>ATA</td>\n","      <td>140</td>\n","      <td>289</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","      <td>172</td>\n","      <td>N</td>\n","      <td>0.0</td>\n","      <td>Up</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>49</td>\n","      <td>F</td>\n","      <td>NAP</td>\n","      <td>160</td>\n","      <td>180</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","      <td>156</td>\n","      <td>N</td>\n","      <td>1.0</td>\n","      <td>Flat</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37</td>\n","      <td>M</td>\n","      <td>ATA</td>\n","      <td>130</td>\n","      <td>283</td>\n","      <td>0</td>\n","      <td>ST</td>\n","      <td>98</td>\n","      <td>N</td>\n","      <td>0.0</td>\n","      <td>Up</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>48</td>\n","      <td>F</td>\n","      <td>ASY</td>\n","      <td>138</td>\n","      <td>214</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","      <td>108</td>\n","      <td>Y</td>\n","      <td>1.5</td>\n","      <td>Flat</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54</td>\n","      <td>M</td>\n","      <td>NAP</td>\n","      <td>150</td>\n","      <td>195</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","      <td>122</td>\n","      <td>N</td>\n","      <td>0.0</td>\n","      <td>Up</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n","0   40   M           ATA        140          289          0     Normal    172   \n","1   49   F           NAP        160          180          0     Normal    156   \n","2   37   M           ATA        130          283          0         ST     98   \n","3   48   F           ASY        138          214          0     Normal    108   \n","4   54   M           NAP        150          195          0     Normal    122   \n","\n","  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n","0              N      0.0       Up             0  \n","1              N      1.0     Flat             1  \n","2              N      0.0       Up             0  \n","3              Y      1.5     Flat             1  \n","4              N      0.0       Up             0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df=pd.read_csv(\"C:/Users/chinm/OneDrive/Documents/Python Scripts/Heart-Dataset Project/heart.csv\")\n","df.head() #reading 5 entries from each column"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.664766Z","iopub.status.busy":"2021-12-13T06:24:53.664122Z","iopub.status.idle":"2021-12-13T06:24:53.672573Z","shell.execute_reply":"2021-12-13T06:24:53.671665Z","shell.execute_reply.started":"2021-12-13T06:24:53.664733Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Age                 int64\n","Sex                object\n","ChestPainType      object\n","RestingBP           int64\n","Cholesterol         int64\n","FastingBS           int64\n","RestingECG         object\n","MaxHR               int64\n","ExerciseAngina     object\n","Oldpeak           float64\n","ST_Slope           object\n","HeartDisease        int64\n","dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.dtypes"]},{"cell_type":"markdown","metadata":{},"source":["As we can see the string data in the dataframe is in the form of object, we need to convert it back to string to work on it"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.675538Z","iopub.status.busy":"2021-12-13T06:24:53.674934Z","iopub.status.idle":"2021-12-13T06:24:53.70147Z","shell.execute_reply":"2021-12-13T06:24:53.700455Z","shell.execute_reply.started":"2021-12-13T06:24:53.675495Z"},"trusted":true},"outputs":[],"source":["string_col = df.select_dtypes(include=\"object\").columns\n","df[string_col]=df[string_col].astype(\"string\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.703732Z","iopub.status.busy":"2021-12-13T06:24:53.702986Z","iopub.status.idle":"2021-12-13T06:24:53.714413Z","shell.execute_reply":"2021-12-13T06:24:53.713513Z","shell.execute_reply.started":"2021-12-13T06:24:53.703691Z"},"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"markdown","metadata":{},"source":["So, as we can see here the object data has been converted to string"]},{"cell_type":"markdown","metadata":{},"source":["## Getting the categorical columns "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.716596Z","iopub.status.busy":"2021-12-13T06:24:53.715969Z","iopub.status.idle":"2021-12-13T06:24:53.724925Z","shell.execute_reply":"2021-12-13T06:24:53.724357Z","shell.execute_reply.started":"2021-12-13T06:24:53.716556Z"},"trusted":true},"outputs":[],"source":["string_col=df.select_dtypes(\"string\").columns.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.72644Z","iopub.status.busy":"2021-12-13T06:24:53.725785Z","iopub.status.idle":"2021-12-13T06:24:53.73486Z","shell.execute_reply":"2021-12-13T06:24:53.734241Z","shell.execute_reply.started":"2021-12-13T06:24:53.726402Z"},"trusted":true},"outputs":[],"source":["num_col=df.columns.to_list()\n","#print(num_col)\n","for col in string_col:\n","    num_col.remove(col)\n","num_col.remove(\"HeartDisease\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.737118Z","iopub.status.busy":"2021-12-13T06:24:53.736525Z","iopub.status.idle":"2021-12-13T06:24:53.781784Z","shell.execute_reply":"2021-12-13T06:24:53.781156Z","shell.execute_reply.started":"2021-12-13T06:24:53.737067Z"},"trusted":true},"outputs":[],"source":["df.describe().T"]},{"cell_type":"markdown","metadata":{},"source":["# The Attributess include:\n","- Age: age of the patient [years]\n","- Sex: sex of the patient [M: Male, F: Female]\n","- ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n","- RestingBP: resting blood pressure [mm Hg]\n","- Cholesterol: serum cholesterol [mm/dl]\n","- FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n","- RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n","- MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n","- ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n","- Oldpeak: oldpeak = ST [Numeric value measured in depression]\n","- ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n","- HeartDisease: output class [1: heart disease, 0: Normal]"]},{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis\n","\n","<img src=\"https://media.giphy.com/media/HUplkVCPY7jTW/giphy.gif\">\n","\n","## First Question should be why do we need this ??\n","\n","Out Come of this phase is as given below : \n","\n","- Understanding the given dataset and helps clean up the given dataset.\n","- It gives you a clear picture of the features and the relationships between them.\n","- Providing guidelines for essential variables and leaving behind/removing non-essential variables.\n","- Handling Missing values or human error.\n","- Identifying outliers.\n","- EDA process would be maximizing insights of a dataset.\n","- This process is time-consuming but very effective,"]},{"cell_type":"markdown","metadata":{},"source":["## Correlation Matrix\n","### Its necessary to remove correlated variables to improve your model.One can find correlations using pandas “.corr()” function and can visualize the correlation matrix using plotly express.\n","- Lighter shades represents positive correlation\n","- Darker shades represents negative correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:53.783262Z","iopub.status.busy":"2021-12-13T06:24:53.782668Z","iopub.status.idle":"2021-12-13T06:24:54.91476Z","shell.execute_reply":"2021-12-13T06:24:54.913855Z","shell.execute_reply.started":"2021-12-13T06:24:53.783224Z"},"trusted":true},"outputs":[],"source":["px.imshow(df.corr(),title=\"Correlation Plot of the Heat Failure Prediction\")"]},{"cell_type":"markdown","metadata":{},"source":["Here we can see Heart Disease has a high negative correlation with \"MaxHR\" and somewhat negative correlation wiht \"Cholesterol\", where as here positive correatlation with \"Oldpeak\",\"FastingBS\" and \"RestingBP\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:54.917901Z","iopub.status.busy":"2021-12-13T06:24:54.917588Z","iopub.status.idle":"2021-12-13T06:24:55.051245Z","shell.execute_reply":"2021-12-13T06:24:55.050353Z","shell.execute_reply.started":"2021-12-13T06:24:54.917868Z"},"trusted":true},"outputs":[],"source":["# Shows the Distribution of Heat Diseases with respect to male and female\n","fig=px.histogram(df, \n","                 x=\"HeartDisease\",\n","                 color=\"Sex\",\n","                 hover_data=df.columns,\n","                 title=\"Distribution of Heart Diseases\",\n","                 barmode=\"group\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:55.052623Z","iopub.status.busy":"2021-12-13T06:24:55.0524Z","iopub.status.idle":"2021-12-13T06:24:55.137753Z","shell.execute_reply":"2021-12-13T06:24:55.136824Z","shell.execute_reply.started":"2021-12-13T06:24:55.052596Z"},"trusted":true},"outputs":[],"source":["fig=px.histogram(df,\n","                 x=\"ChestPainType\",\n","                 color=\"Sex\",\n","                 hover_data=df.columns,\n","                 title=\"Types of Chest Pain\"\n","                )\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:55.139419Z","iopub.status.busy":"2021-12-13T06:24:55.139095Z","iopub.status.idle":"2021-12-13T06:24:55.219925Z","shell.execute_reply":"2021-12-13T06:24:55.219019Z","shell.execute_reply.started":"2021-12-13T06:24:55.13938Z"},"trusted":true},"outputs":[],"source":["fig=px.histogram(df,\n","                 x=\"Sex\",\n","                 hover_data=df.columns,\n","                 title=\"Sex Ratio in the Data\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:55.22172Z","iopub.status.busy":"2021-12-13T06:24:55.221403Z","iopub.status.idle":"2021-12-13T06:24:55.300794Z","shell.execute_reply":"2021-12-13T06:24:55.300117Z","shell.execute_reply.started":"2021-12-13T06:24:55.22168Z"},"trusted":true},"outputs":[],"source":["fig=px.histogram(df,\n","                 x=\"RestingECG\",\n","                 hover_data=df.columns,\n","                 title=\"Distribution of Resting ECG\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. This shows the relationship for (n, 2) combination of variable in a DataFrame as a matrix of plots and the diagonal plots are the univariate plots."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:24:55.302459Z","iopub.status.busy":"2021-12-13T06:24:55.302219Z","iopub.status.idle":"2021-12-13T06:25:08.152054Z","shell.execute_reply":"2021-12-13T06:25:08.151284Z","shell.execute_reply.started":"2021-12-13T06:24:55.302429Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,10))\n","sns.pairplot(df,hue=\"HeartDisease\")\n","plt.title(\"Looking for Insites in Data\")\n","plt.legend(\"HeartDisease\")\n","plt.tight_layout()\n","plt.plot()"]},{"cell_type":"markdown","metadata":{},"source":["### Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:08.153867Z","iopub.status.busy":"2021-12-13T06:25:08.153162Z","iopub.status.idle":"2021-12-13T06:25:12.20015Z","shell.execute_reply":"2021-12-13T06:25:12.199516Z","shell.execute_reply.started":"2021-12-13T06:25:08.153832Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,10))\n","for i,col in enumerate(df.columns,1):\n","    plt.subplot(4,3,i)\n","    plt.title(f\"Distribution of {col} Data\")\n","    sns.histplot(df[col],kde=True)\n","    plt.tight_layout()\n","    plt.plot()\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["# Outliers\n","\n","A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables.The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary:\n","- Minimum\n","- First quartile\n","- Median\n","- Third quartile\n","- Maximum.\n","\n","In the simplest box plot the central rectangle spans the first quartile to the third quartile (the interquartile range or IQR).A segment inside the rectangle shows the median and “whiskers” above and below the box show the locations of the minimum and maximum."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.201666Z","iopub.status.busy":"2021-12-13T06:25:12.20129Z","iopub.status.idle":"2021-12-13T06:25:12.292689Z","shell.execute_reply":"2021-12-13T06:25:12.291973Z","shell.execute_reply.started":"2021-12-13T06:25:12.201626Z"},"trusted":true},"outputs":[],"source":["\n","fig = px.box(df,y=\"Age\",x=\"HeartDisease\",title=f\"Distrubution of Age\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.294346Z","iopub.status.busy":"2021-12-13T06:25:12.293821Z","iopub.status.idle":"2021-12-13T06:25:12.368804Z","shell.execute_reply":"2021-12-13T06:25:12.367979Z","shell.execute_reply.started":"2021-12-13T06:25:12.294315Z"},"trusted":true},"outputs":[],"source":["fig = px.box(df,y=\"RestingBP\",x=\"HeartDisease\",title=f\"Distrubution of RestingBP\",color=\"Sex\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.370119Z","iopub.status.busy":"2021-12-13T06:25:12.36991Z","iopub.status.idle":"2021-12-13T06:25:12.435667Z","shell.execute_reply":"2021-12-13T06:25:12.434686Z","shell.execute_reply.started":"2021-12-13T06:25:12.370094Z"},"trusted":true},"outputs":[],"source":["fig = px.box(df,y=\"Cholesterol\",x=\"HeartDisease\",title=f\"Distrubution of Cholesterol\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.437438Z","iopub.status.busy":"2021-12-13T06:25:12.437191Z","iopub.status.idle":"2021-12-13T06:25:12.502247Z","shell.execute_reply":"2021-12-13T06:25:12.501565Z","shell.execute_reply.started":"2021-12-13T06:25:12.437409Z"},"trusted":true},"outputs":[],"source":["fig = px.box(df,y=\"Oldpeak\",x=\"HeartDisease\",title=f\"Distrubution of Oldpeak\")\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.503947Z","iopub.status.busy":"2021-12-13T06:25:12.503547Z","iopub.status.idle":"2021-12-13T06:25:12.570452Z","shell.execute_reply":"2021-12-13T06:25:12.569666Z","shell.execute_reply.started":"2021-12-13T06:25:12.503918Z"},"trusted":true},"outputs":[],"source":["fig = px.box(df,y=\"MaxHR\",x=\"HeartDisease\",title=f\"Distrubution of MaxHR\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing\n","Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n","\n","The concepts that I will cover in this article are\n","1. Handling Null Values\n","2. Feature Scaling\n","3. Handling Categorical Variables"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Handling Null Values : \n","In any real-world dataset, there are always few null values. It doesn’t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.\n","\n","> In python NULL is reprsented with NaN. So don’t get confused between these two,they can be used interchangably.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.572468Z","iopub.status.busy":"2021-12-13T06:25:12.571986Z","iopub.status.idle":"2021-12-13T06:25:12.589891Z","shell.execute_reply":"2021-12-13T06:25:12.588877Z","shell.execute_reply.started":"2021-12-13T06:25:12.572424Z"},"trusted":true},"outputs":[],"source":["# Checking for Type of data\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.591566Z","iopub.status.busy":"2021-12-13T06:25:12.591066Z","iopub.status.idle":"2021-12-13T06:25:12.601357Z","shell.execute_reply":"2021-12-13T06:25:12.600466Z","shell.execute_reply.started":"2021-12-13T06:25:12.591529Z"},"trusted":true},"outputs":[],"source":["# Checking for NULLs in the data\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["So we can see our data does not have any null values\n","but in case we have missing values, we can remove the data as well.\n","\n","However, it is not the best option to remove the rows and columns from our dataset as it can result in significant information loss. If you have 300K data points then removing 2–3 rows won’t affect your dataset much but if you only have 100 data points and out of which 20 have NaN values for a particular field then you can’t simply drop those rows. In real-world datasets, it can happen quite often that you have a large number of NaN values for a particular field.\n","Ex — Suppose we are collecting the data from a survey, then it is possible that there could be an optional field which let’s say 20% of people left blank. So when we get the dataset then we need to understand that the remaining 80% of data is still useful, so rather than dropping these values we need to somehow substitute the missing 20% values. We can do this with the help of Imputation.\n","\n","### Imputation:\n","\n","Imputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn.\n","\n","For example : \n","\n","```python\n","from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer = imputer.fit(df[['Weight']])\n","df['Weight'] = imputer.transform(df[['Weight']])\n","\n","```\n","\n","### As we do not have any missing data so we will not be using this approch"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Feature Scaling\n"]},{"cell_type":"markdown","metadata":{},"source":["### Why Should we Use Feature Scaling?\n","\n","The first question we need to address – why do we need to scale the variables in our dataset? Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Let me explain that in more detail.\n","\n","## 1. Distance Based Algorithms : \n","    \n","Distance algorithms like **\"KNN\"**, **\"K-means\"** and **\"SVM\"** are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n","Whem two features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature.\n","\n","Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result.\n","\n","<img src=\"https://miro.medium.com/max/1000/0*MZKG8sTIdSNv6TXB\" width=50%>\n","\n","\n","## 2. Tree-Based Algorithms : \n","\n","Tree-based algorithms, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. This split on a feature is not influenced by other features.\n","\n","So, there is virtually no effect of the remaining features on the split. This is what makes them invariant to the scale of the features!\n","\n","<img src=\"https://miro.medium.com/max/925/0*U0rcW7XrdHpvI0hU.jpeg\" width=70%>"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-12-12T10:03:15.924676Z","iopub.status.busy":"2021-12-12T10:03:15.924288Z","iopub.status.idle":"2021-12-12T10:03:15.941799Z","shell.execute_reply":"2021-12-12T10:03:15.940934Z","shell.execute_reply.started":"2021-12-12T10:03:15.924636Z"}},"source":["### What is Normalization?\n","\n","Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n","\n","Here's the fromula for normalization : \n","\n","<img src=\"https://i.stack.imgur.com/EuitP.png\" width=40%>\n","\n","Here, Xmax and Xmin are the maximum and the minimum values of the feature respectively.\n","\n","When the value of X is the minimum value in the column, the numerator will be 0, and hence X’ is 0\n","On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator and thus the value of X’ is 1\n","If the value of X is between the minimum and the maximum value, then the value of X’ is between 0 and 1"]},{"cell_type":"markdown","metadata":{},"source":["## What is Standardization?\n","\n","Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation\n","\n","Here's the formula for Standarization:\n","\n","<img src=\"https://clavelresearch.files.wordpress.com/2019/03/z-score-population.png\" width=30%>\n","\n","## The Big Question – Normalize or Standardize?\n","\n","Normalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n","\n","- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n","\n","- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n","\n","However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. "]},{"cell_type":"markdown","metadata":{},"source":["### Robust Scaler\n","When working with outliers we can use Robust Scaling for scakling our data,\n","It scales features using statistics that are robust to outliers. This method removes the median and scales the data in the range between 1st quartile and 3rd quartile. i.e., in between 25th quantile and 75th quantile range. This range is also called an Interquartile range. \n","The median and the interquartile range are then stored so that it could be used upon future data using the transform method. If outliers are present in the dataset, then the median and the interquartile range provide better results and outperform the sample mean and variance. \n","RobustScaler uses the interquartile range so that it is robust to outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:12.603284Z","iopub.status.busy":"2021-12-13T06:25:12.60303Z","iopub.status.idle":"2021-12-13T06:25:13.364331Z","shell.execute_reply":"2021-12-13T06:25:13.363382Z","shell.execute_reply.started":"2021-12-13T06:25:12.603257Z"},"trusted":true},"outputs":[],"source":["# data\n","x = pd.DataFrame({\n","    # Distribution with lower outliers\n","    'x1': np.concatenate([np.random.normal(20, 2, 1000), np.random.normal(1, 2, 25)]),\n","    # Distribution with higher outliers\n","    'x2': np.concatenate([np.random.normal(30, 2, 1000), np.random.normal(50, 2, 25)]),\n","})\n","np.random.normal\n"," \n","scaler = preprocessing.RobustScaler()\n","robust_df = scaler.fit_transform(x)\n","robust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n"," \n","scaler = preprocessing.StandardScaler()\n","standard_df = scaler.fit_transform(x)\n","standard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n"," \n","scaler = preprocessing.MinMaxScaler()\n","minmax_df = scaler.fit_transform(x)\n","minmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n"," \n","fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\n","ax1.set_title('Before Scaling')\n"," \n","sns.kdeplot(x['x1'], ax = ax1, color ='r')\n","sns.kdeplot(x['x2'], ax = ax1, color ='b')\n","ax2.set_title('After Robust Scaling')\n"," \n","sns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\n","sns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\n","ax3.set_title('After Standard Scaling')\n"," \n","sns.kdeplot(standard_df['x1'], ax = ax3, color ='black')\n","sns.kdeplot(standard_df['x2'], ax = ax3, color ='g')\n","ax4.set_title('After Min-Max Scaling')\n"," \n","sns.kdeplot(minmax_df['x1'], ax = ax4, color ='black')\n","sns.kdeplot(minmax_df['x2'], ax = ax4, color ='g')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Handling Categorical Variables\n","\n","Categorical variables/features are any feature type can be classified into two major types:\n","- Nominal\n","- Ordinal\n","\n","Nominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.Ordinal variables, on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.\n","\n","It is a binary classification problem:\n","the target here is **not skewed** but we use the best metric for this binary classification problem which would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.\n","\n","We have to know that computers do not understand text data and thus, we need to convert these categories to numbers. A simple way of doing that can be to use :\n","- Label Encoding\n","```python\n","from sklearn.preprocessing import LabelEncoder\n","```\n","- One Hot Encoding\n","```python\n","pd.get_dummies()\n","```\n","\n","but we need to understand where to use which type of label encoding:\n","\n","**For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding**\n","- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. \n","- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature\n","\n","**For Tree based Machine Learning Algorithms the best way to go is with Label Encoding**\n","\n","- LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.365873Z","iopub.status.busy":"2021-12-13T06:25:13.365611Z","iopub.status.idle":"2021-12-13T06:25:13.383826Z","shell.execute_reply":"2021-12-13T06:25:13.382727Z","shell.execute_reply.started":"2021-12-13T06:25:13.365842Z"},"trusted":true},"outputs":[],"source":["df[string_col].head()\n","for col in string_col:\n","    print(f\"The distribution of categorical valeus in the {col} is : \")\n","    print(df[col].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.386051Z","iopub.status.busy":"2021-12-13T06:25:13.385395Z","iopub.status.idle":"2021-12-13T06:25:13.405358Z","shell.execute_reply":"2021-12-13T06:25:13.404467Z","shell.execute_reply.started":"2021-12-13T06:25:13.386005Z"},"trusted":true},"outputs":[],"source":["# As we will be using both types of approches for demonstration lets do First Label Ecoding \n","# which will be used with Tree Based Algorthms\n","df_tree = df.apply(LabelEncoder().fit_transform)\n","df_tree.head()"]},{"cell_type":"markdown","metadata":{},"source":["We can use this directly in many tree-based models:\n","- Decision trees\n","- Random forest\n","- Extra Trees\n","- Or any kind of boosted trees model\n","    - XGBoost\n","    - GBM\n","    - LightGBM\n","    \n","This type of encoding cannot be used in linear models, support vector machines or neural networks as they expect data to be normalized (or standardized). For these types of models, we can binarize the data.\n","As shown bellow : "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.408094Z","iopub.status.busy":"2021-12-13T06:25:13.407862Z","iopub.status.idle":"2021-12-13T06:25:13.439913Z","shell.execute_reply":"2021-12-13T06:25:13.439054Z","shell.execute_reply.started":"2021-12-13T06:25:13.408067Z"},"trusted":true},"outputs":[],"source":["## Creaeting one hot encoded features for working with non tree based algorithms \n","df_nontree=pd.get_dummies(df,columns=string_col,drop_first=False)\n","df_nontree.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.442837Z","iopub.status.busy":"2021-12-13T06:25:13.442075Z","iopub.status.idle":"2021-12-13T06:25:13.468562Z","shell.execute_reply":"2021-12-13T06:25:13.467694Z","shell.execute_reply.started":"2021-12-13T06:25:13.442793Z"},"trusted":true},"outputs":[],"source":["# Getting the target column at the end\n","target=\"HeartDisease\"\n","y=df_nontree[target].values\n","df_nontree.drop(\"HeartDisease\",axis=1,inplace=True)\n","df_nontree=pd.concat([df_nontree,df[target]],axis=1)\n","df_nontree.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Chossing the right Cross-Validation"]},{"cell_type":"markdown","metadata":{},"source":["Choosing the right cross-validation depends on the dataset you are dealing with, and one’s choice of cross-validation on one dataset may or may not apply to other datasets. However, there are a few types of cross-validation techniques which are the most popular and widely used.\n","These include:\n","\n","- k-fold cross-validation\n","- stratified k-fold cross-validation\n","Cross-validation is dividing training data into a few parts. We train the model on some of these parts and test on the remaining parts\n","<br>\n","\n","<img src=\"https://i.stack.imgur.com/8uEci.png\" width=50%>\n","<br>\n","\n","## 1. K-fold cross-validation :\n","\n","As you can see, we divide the samples and the targets associated with them. We can divide the data into k different sets which are exclusive of each other. This is known as k-fold cross-validation, We can split any data into k-equal parts using KFold from scikit-learn. Each sample is assigned a value from 0 to k-1 when using k-fold cross validation.\n","\n","## 2. Stratified k-fold cross-validation :\n","\n","If you have a skewed dataset for binary classification with 90% positive samples and only 10% negative samples, you don't want to use random k-fold cross-validation. Using simple k-fold cross-validation for a dataset like this can result in folds with all negative samples. In these cases, we prefer using stratified k-fold cross-validation. Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So, in each fold, you will have the same 90% positive and 10% negative samples. Thus, whatever metric you choose to evaluate, it will give similar results across all folds.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training our Machine Learning Model : \n"]},{"cell_type":"markdown","metadata":{},"source":["# NON-TREE BASED ALGORITHMS\n","\n","<img src=\"https://media.giphy.com/media/zMukICnMEZmSf8zvXd/giphy.gif\">"]},{"cell_type":"markdown","metadata":{},"source":["So as have talked earlier we have to use differnt ways to works with categorical data, so we will be using different methods:\n","\n","## 1.Using Logistic Regression : \n","\n","Logistic regression is a calculation used to predict a binary outcome: either something happens, or does not. This can be exhibited as Yes/No, Pass/Fail, Alive/Dead, etc. \n","\n","Independent variables are analyzed to determine the binary outcome with the results falling into one of two categories. The independent variables can be categorical or numeric, but the dependent variable is always categorical. Written like this:\n","\n","P(Y=1|X) or P(Y=0|X)\n","\n","It calculates the probability of dependent variable Y, given independent variable X. \n","\n","This can be used to calculate the probability of a word having a positive or negative connotation (0, 1, or on a scale between). Or it can be used to determine the object contained in a photo (tree, flower, grass, etc.), with each object given a probability between 0 and 1.\n","\n","<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.473647Z","iopub.status.busy":"2021-12-13T06:25:13.473215Z","iopub.status.idle":"2021-12-13T06:25:13.47836Z","shell.execute_reply":"2021-12-13T06:25:13.477608Z","shell.execute_reply.started":"2021-12-13T06:25:13.473599Z"},"trusted":true},"outputs":[],"source":["feature_col_nontree=df_nontree.columns.to_list()\n","feature_col_nontree.remove(target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.480214Z","iopub.status.busy":"2021-12-13T06:25:13.479799Z","iopub.status.idle":"2021-12-13T06:25:13.852581Z","shell.execute_reply":"2021-12-13T06:25:13.851737Z","shell.execute_reply.started":"2021-12-13T06:25:13.480151Z"},"trusted":true},"outputs":[],"source":["from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\n","from sklearn.preprocessing import RobustScaler,MinMaxScaler,StandardScaler\n","acc_log=[]\n","\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    #print(pd.DataFrame(X_valid).head())\n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    \n","    clf=LogisticRegression()\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_log.append(acc)\n","    print(f\"The accuracy for Fold {fold+1} : {acc}\")\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["## 2.Using Naive Bayers\n","\n","<img src=\"https://insightimi.files.wordpress.com/2020/04/unnamed-1.png\">"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.854522Z","iopub.status.busy":"2021-12-13T06:25:13.853907Z","iopub.status.idle":"2021-12-13T06:25:13.985275Z","shell.execute_reply":"2021-12-13T06:25:13.984555Z","shell.execute_reply.started":"2021-12-13T06:25:13.854475Z"},"trusted":true},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","acc_Gauss=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=GaussianNB()\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_Gauss.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["## 3.Using SVM(Support Vector Machines):\n","\n","A support vector machine (SVM) uses algorithms to train and classify data within degrees of polarity, taking it to a degree beyond X/Y prediction. \n","\n","For a simple visual explanation, we’ll use two tags: red and blue, with two data features: X and Y, then train our classifier to output an X/Y coordinate as either red or blue.\n","\n","<img src=\"https://monkeylearn.com/static/93a102a9b7b96d9047212e15b627724b/d8712/image4-3.webp\" width=40%>\n","\n","The SVM then assigns a hyperplane that best separates the tags. In two dimensions this is simply a line. Anything on one side of the line is red and anything on the other side is blue. In sentiment analysis, for example, this would be positive and negative.\n","\n","In order to maximize machine learning, the best hyperplane is the one with the largest distance between each tag:\n","\n","<img src=\"https://monkeylearn.com/static/e662f65502ffd24d3ee23c07efe88d9e/d8712/image3-2.webp\" width=40%>\n","\n","However, as data sets become more complex, it may not be possible to draw a single line to classify the data into two camps:\n","\n","<img src=\"https://monkeylearn.com/static/5db2d9178789315ce9fa42f579c895a6/93a24/image2-3.webp\" width=40%>\n","\n","Using SVM, the more complex the data, the more accurate the predictor will become. Imagine the above in three dimensions, with a Z-axis added, so it becomes a circle.\n","\n","Mapped back to two dimensions with the best hyperplane, it looks like this\n","\n","<img src=\"https://monkeylearn.com/static/583405ebadf21c9691030ec4bb875e48/93a24/image6-2.webp\" width=40%>\n","\n","SVM allows for more accurate machine learning because it’s multidimensional.\n","\n","We need to choose the best Kernel according to our need.\n","- The linear kernel is mostly preferred for text classification problems as it performs well for large datasets.\n","- Gaussian kernels tend to give good results when there is no additional information regarding data that is not available.\n","- Rbf kernel is also a kind of Gaussian kernel which projects the high dimensional data and then searches a linear separation for it.\n","- Polynomial kernels give good results for problems where all the training data is normalized."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:13.986714Z","iopub.status.busy":"2021-12-13T06:25:13.986392Z","iopub.status.idle":"2021-12-13T06:25:14.123747Z","shell.execute_reply":"2021-12-13T06:25:14.122901Z","shell.execute_reply.started":"2021-12-13T06:25:13.986687Z"},"trusted":true},"outputs":[],"source":["# Using Linear Kernel\n","from sklearn.svm import SVC\n","acc_svm=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=SVC(kernel=\"linear\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_svm.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:14.125442Z","iopub.status.busy":"2021-12-13T06:25:14.125111Z","iopub.status.idle":"2021-12-13T06:25:14.318971Z","shell.execute_reply":"2021-12-13T06:25:14.318033Z","shell.execute_reply.started":"2021-12-13T06:25:14.125399Z"},"trusted":true},"outputs":[],"source":["## Using Sigmoid Kernel\n","from sklearn.svm import SVC\n","acc_svm_sig=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=SVC(kernel=\"sigmoid\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_svm_sig.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:14.32099Z","iopub.status.busy":"2021-12-13T06:25:14.320482Z","iopub.status.idle":"2021-12-13T06:25:14.476021Z","shell.execute_reply":"2021-12-13T06:25:14.475125Z","shell.execute_reply.started":"2021-12-13T06:25:14.32095Z"},"trusted":true},"outputs":[],"source":["## Using RBF kernel\n","from sklearn.svm import SVC\n","acc_svm_rbf=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=SVC(kernel=\"rbf\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_svm_rbf.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:14.477662Z","iopub.status.busy":"2021-12-13T06:25:14.477422Z","iopub.status.idle":"2021-12-13T06:25:14.63591Z","shell.execute_reply":"2021-12-13T06:25:14.635066Z","shell.execute_reply.started":"2021-12-13T06:25:14.477634Z"},"trusted":true},"outputs":[],"source":["## Using RBF kernel\n","from sklearn.svm import SVC\n","acc_svm_poly=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=SVC(kernel=\"poly\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_svm_poly.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["## Using K-nearest Neighbors\n","The optimal K value usually found is the square root of N, where N is the total number of samples\n","\n","\n","K-nearest neighbors (k-NN) is a pattern recognition algorithm that uses training datasets to find the k closest relatives in future examples. \n","\n","When k-NN is used in classification, you calculate to place data within the category of its nearest neighbor. If k = 1, then it would be placed in the class nearest 1. K is classified by a plurality poll of its neighbors.\n","\n","<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:14.637601Z","iopub.status.busy":"2021-12-13T06:25:14.63737Z","iopub.status.idle":"2021-12-13T06:25:14.86269Z","shell.execute_reply":"2021-12-13T06:25:14.861709Z","shell.execute_reply.started":"2021-12-13T06:25:14.637572Z"},"trusted":true},"outputs":[],"source":["## Using RBF kernel\n","from sklearn.neighbors import KNeighborsClassifier\n","acc_KNN=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n","    \n","    X_train=df_nontree.loc[trn_,feature_col_nontree]\n","    y_train=df_nontree.loc[trn_,target]\n","    \n","    X_valid=df_nontree.loc[val_,feature_col_nontree]\n","    y_valid=df_nontree.loc[val_,target]\n","    \n","    ro_scaler=MinMaxScaler()\n","    X_train=ro_scaler.fit_transform(X_train)\n","    X_valid=ro_scaler.transform(X_valid)\n","    \n","    clf=KNeighborsClassifier(n_neighbors=32)\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_KNN.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","    \n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["# TREE BASED ALGORITHM \n","<img src=\"https://media.giphy.com/media/IyZM6HFe2zgrK/giphy.gif\">"]},{"cell_type":"markdown","metadata":{},"source":["## Using Decission tree Classifier\n","A decision tree is a supervised learning algorithm that is perfect for classification problems, as it’s able to order classes on a precise level. It works like a flow chart, separating data points into two similar categories at a time from the “tree trunk” to “branches,” to “leaves,” where the categories become more finitely similar. This creates categories within categories, allowing for organic classification with limited human supervision."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:25:14.864354Z","iopub.status.busy":"2021-12-13T06:25:14.864035Z","iopub.status.idle":"2021-12-13T06:25:14.868935Z","shell.execute_reply":"2021-12-13T06:25:14.867971Z","shell.execute_reply.started":"2021-12-13T06:25:14.864312Z"},"trusted":true},"outputs":[],"source":["feature_col_tree=df_tree.columns.to_list()\n","feature_col_tree.remove(target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:54:58.121148Z","iopub.status.busy":"2021-12-13T06:54:58.119708Z","iopub.status.idle":"2021-12-13T06:54:58.289541Z","shell.execute_reply":"2021-12-13T06:54:58.288256Z","shell.execute_reply.started":"2021-12-13T06:54:58.12103Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","acc_Dtree=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_tree,y=y)):\n","    \n","    X_train=df_tree.loc[trn_,feature_col_tree]\n","    y_train=df_tree.loc[trn_,target]\n","    \n","    X_valid=df_tree.loc[val_,feature_col_tree]\n","    y_valid=df_tree.loc[val_,target]\n","    \n","    clf=DecisionTreeClassifier(criterion=\"entropy\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_Dtree.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:56:11.090696Z","iopub.status.busy":"2021-12-13T06:56:11.089816Z","iopub.status.idle":"2021-12-13T06:56:11.965711Z","shell.execute_reply":"2021-12-13T06:56:11.964696Z","shell.execute_reply.started":"2021-12-13T06:56:11.090656Z"},"trusted":true},"outputs":[],"source":["import graphviz\n","from sklearn import tree\n","# DOT data\n","dot_data = tree.export_graphviz(clf, out_file=None, \n","                                feature_names=feature_col_tree,  \n","                                class_names=target,\n","                                filled=True)\n","\n","# Draw graph\n","graph = graphviz.Source(dot_data, format=\"png\") \n","graph"]},{"cell_type":"markdown","metadata":{},"source":["## Using Random Forest Classifier\n","\n","Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction (see figure below).\n","\n","<img src=\"https://miro.medium.com/max/5752/1*5dq_1hnqkboZTcKFfwbO9A.png\" width=70%>\n","\n","The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:\n","\n",">A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n","\n","The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don’t constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:58:48.838734Z","iopub.status.busy":"2021-12-13T06:58:48.838287Z","iopub.status.idle":"2021-12-13T06:58:51.449926Z","shell.execute_reply":"2021-12-13T06:58:51.449125Z","shell.execute_reply.started":"2021-12-13T06:58:48.838704Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","acc_RandF=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_tree,y=y)):\n","    \n","    X_train=df_tree.loc[trn_,feature_col_tree]\n","    y_train=df_tree.loc[trn_,target]\n","    \n","    X_valid=df_tree.loc[val_,feature_col_tree]\n","    y_valid=df_tree.loc[val_,target]\n","    \n","    clf=RandomForestClassifier(n_estimators=200,criterion=\"entropy\")\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_RandF.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T06:28:26.807337Z","iopub.status.busy":"2021-12-13T06:28:26.807052Z","iopub.status.idle":"2021-12-13T06:28:27.189819Z","shell.execute_reply":"2021-12-13T06:28:27.18921Z","shell.execute_reply.started":"2021-12-13T06:28:26.807307Z"},"trusted":true},"outputs":[],"source":["## Checking Feature importance \n","\n","plt.figure(figsize=(20,15))\n","importance = clf.feature_importances_\n","idxs = np.argsort(importance)\n","plt.title(\"Feature Importance\")\n","plt.barh(range(len(idxs)),importance[idxs],align=\"center\")\n","plt.yticks(range(len(idxs)),[feature_col_tree[i] for i in idxs])\n","plt.xlabel(\"Random Forest Feature Importance\")\n","#plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Using XGBoost\n","\n","Unlike many other algorithms, XGBoost is an ensemble learning algorithm meaning that it combines the results of many models, called base learners to make a prediction.\n","\n","Just like in Random Forests, XGBoost uses Decision Trees as base learners:\n","\n","\n","\n","However, the trees used by XGBoost are a bit different than traditional decision trees. They are called CART trees (Classification and Regression trees) and instead of containing a single decision in each “leaf” node, they contain real-value scores of whether an instance belongs to a group. After the tree reaches max depth, the decision can be made by converting the scores into categories using a certain threshold."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T07:35:23.724615Z","iopub.status.busy":"2021-12-13T07:35:23.72433Z","iopub.status.idle":"2021-12-13T07:35:25.319586Z","shell.execute_reply":"2021-12-13T07:35:25.318707Z","shell.execute_reply.started":"2021-12-13T07:35:23.724585Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier\n","acc_XGB=[]\n","kf=model_selection.StratifiedKFold(n_splits=5)\n","for fold , (trn_,val_) in enumerate(kf.split(X=df_tree,y=y)):\n","    \n","    X_train=df_tree.loc[trn_,feature_col_tree]\n","    y_train=df_tree.loc[trn_,target]\n","    \n","    X_valid=df_tree.loc[val_,feature_col_tree]\n","    y_valid=df_tree.loc[val_,target]\n","    \n","    clf=XGBClassifier()\n","    clf.fit(X_train,y_train)\n","    y_pred=clf.predict(X_valid)\n","    print(f\"The fold is : {fold} : \")\n","    print(classification_report(y_valid,y_pred))\n","    acc=roc_auc_score(y_valid,y_pred)\n","    acc_XGB.append(acc)\n","    print(f\"The accuracy for {fold+1} : {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T07:37:50.31508Z","iopub.status.busy":"2021-12-13T07:37:50.314494Z","iopub.status.idle":"2021-12-13T07:37:51.959147Z","shell.execute_reply":"2021-12-13T07:37:51.958266Z","shell.execute_reply.started":"2021-12-13T07:37:50.315047Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(30, 30))\n","from xgboost import plot_tree\n","plot_tree(clf,num_trees=0,rankdir=\"LR\",ax=ax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Choosing the best Evaluation Matrix:\n","\n","If we talk about classification problems, the most common metrics used are:\n","- Accuracy\n","- Precision (P)\n","- Recall (R)\n","- F1 score (F1)\n","- Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC):\n","    - If we calculate the area under the ROC curve, we are calculating another metric which is used very often when you have a dataset which has skewed binary targets. This metric is known as the Area Under ROC Curve or Area Under Curve or just simply AUC. There are many ways to calculate the area under the ROC curve\n","    - AUC is a widely used metric for skewed binary classification tasks in the industry,and a metric everyone should know about\n","\n","- Log loss\n","    > Log Loss = - 1.0 * ( target * log(prediction) + (1 - target) * log(1 - prediction) )\n","\n","Most of the metrics that we discussed until now can be converted to a multi-class version. The idea is quite simple. Let’s take precision and recall. We can calculate precision and recall for each class in a multi-class classification problem\n","\n","- **Mcro averaged precision**: calculate precision for all classes individually and then average them\n","- **Micro averaged precision**: calculate class wise true positive and false positive and then use that to calculate overall precision\n","- **Weighted precision**: same as macro but in this case, it is weighted average depending on the number of items in each class\n","\n","<img src=\"https://cdn-images-1.medium.com/max/800/1*1WPbfzztdv50V22TpA6njw.png\">\n"]},{"cell_type":"markdown","metadata":{},"source":["# Further Readings if interested!!!!\n","\n","I have created these notebooks as well for anyone who is intereseted in learning new things in the domain of machine learning application. Please refer to them if interested.😁\n","\n","- Notebook : Automating the Machine Learning workflow using [AutoXGB : XGBoost + Optuna](https://www.kaggle.com/durgancegaur/autoxgb-xgboost-optuna-score-0-95437-dec) \n","- Notebook : [Working with CatBoost](https://www.kaggle.com/durgancegaur/tps-dec-hyperp-tuning-catboost-score-0-95451)\n","- Notebook : Working with data imbalance and [upsampling of data SMOTE](https://www.kaggle.com/durgancegaur/working-with-data-imbalance-eda-99-auc)\n","- Notebook : Working wtih [H2OAuoML](https://www.kaggle.com/durgancegaur/eda-and-working-with-h2oautoml)\n","\n","\n","<img src=\"https://media.giphy.com/media/wsWaiO3gBYXyZeq9v2/giphy.gif\">"]},{"cell_type":"raw","metadata":{},"source":["# Thanks for reading till the end ! If you liked the notebook pls do Upvote👍 and give some remarks/advice if you feels some things need to be added😁😁😁"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
